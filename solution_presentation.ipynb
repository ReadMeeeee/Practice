{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Практика\n",
    "\n",
    "## Задача\n",
    "\n",
    "Есть переписка ТП с пользователем.\n",
    "Переписка содержит все данные по решению инцидента\n",
    "- описание и уточнение проблемы,\n",
    "- обсуждение вариантов решения,\n",
    "- итоговое решение\n",
    "\n",
    "На основе переписки формировать два текста:\n",
    "1. краткое описание проблемы (возможно, с выделением ключевых слов),\n",
    "2. шаги решения (инструкция).\n",
    "\n",
    "Будет использоваться механизм поиска похожих обращений (по описанию проблемы)\n",
    "\n",
    "## Ожидаемое решение\n",
    "\n",
    "Функция принимающая на вход `.docx`-файлы, и возвращающая массив решений (и пусть директорию решений в `.txt`-файлах)."
   ],
   "id": "944e57bc8df88fb9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Этапы решения задачи.\n",
    "\n",
    "### 1. Загрузка `.docx`-документов.\n",
    "\n",
    "Используя библиотеку `python-docx` для обработки Word-документов, описана функция `load_docx_from_project(path)` для гибкой загрузки всех `.docx`-файлов из заданной директории, от моментального форматирования в класс `Chat` отказался в пользу масштабируемости, чтобы на руках были сырые данные.\n",
    "\n",
    "### 2. Представление и формат данных.\n",
    "\n",
    "Данные в переписке представлены как таблицы с колонками:\n",
    "- Номер обращения\n",
    "- Отправитель\n",
    "- Текст сообщения\n",
    "\n",
    "Решение строится на 3х ключевых классах:\n",
    "\n",
    "- `Message`: инкапсулирует отправителя и текст.\n",
    "- `Chat`: объединяет список сообщений и метаданные (название, номера обращений).\n",
    "- `CompanyChat`: содержит очищенный текст чата, готовый к отправке в ИИ.\n",
    "\n",
    "**Плюсы:**\n",
    "- Чёткое разделение областей ответственности.\n",
    "- Упрощает последующую обработку данных.\n",
    "- Повышает читаемость и поддержку кода.\n",
    "Отсюда вытекает небольшой выигрыш в гибкости и мобильности кода.\n",
    "\n",
    "### 3. Очистка и нормализация текста.\n",
    "\n",
    "Перед обработкой ИИ, данные проходят классическую фильтрацию:\n",
    "- Удаление спецсимволов.\n",
    "- Очистка пустых строк.\n",
    "- Приведение регистра (опционально).\n",
    "\n",
    "\n",
    "### 4. Подготовка промптов для ИИ.\n",
    "\n",
    "Для каждого типа запроса формируется `LLMRequest` на базе `InstructionBlock`, который в свою очередь будет формироваться из `.json`-файла содержащего:\n",
    "- Роль (например: \"Ты — ассистент, анализирующий переписку...\")\n",
    "- Инструкцию\n",
    "- Контекст (например, список аббревиатур)\n",
    "- Формат ответа\n",
    "- `max_tokens` — ограничение длины ответа\n",
    "Такой подход должен обеспечить возможность к будущему расширению и простоте управления (исправления).\n",
    "\n",
    "### 5. Отправка запроса и обработка со стороны ИИ.\n",
    "\n",
    "Создается запрос к заданному ИИ с инструктивным промптом к задаче. Использован `ThreadPoolExecutor`, чтобы одновременно отправлять три запроса:\n",
    "- Краткое описание\n",
    "- Ключевые слова\n",
    "- Решение проблемы\n",
    "Выбор пал именно на `ThreadPoolExecutor` так как в отличии, например, от `Threading` дает простое управление потоками, и главное - его не блокирует GIL, так как выполняются операции I/O.\n",
    "\n",
    "**Плюсы:**\n",
    "- Экономия времени при большом числе файлов.\n",
    "- Возможность легко отключить параллельность для отладки (флаг `multi_thread`).\n",
    "\n",
    "### 6. Сохранение результатов и финальная обертка.\n",
    "\n",
    "После обработки создаётся объект `ProblemWithSolution`, в который входят:\n",
    "- Название компании\n",
    "- Краткое описание\n",
    "- Ключевые слова\n",
    "- Инструкция\n",
    "\n",
    "Отдельно от `.ipynb`-решения создам модуль, с функцией, который будет на вход получать директорию и возвращать директорию с `.txt` решениями.\n",
    "\n",
    "## Заключение.\n",
    "\n",
    "Так как задача оказалось достаточно простой, я прибегнул к упрощению и оптимизации отдельных фрагментов. Таким образом удалось достичь:\n",
    "- **Гибкости**: в код достаточно много разделений на отдельный элементы, классы или функции, что позволит адаптировать под новые задачи.\n",
    "- **Масштабируемости**: реализация сразу предполагает работу как с одним документом, так и с массивом.\n",
    "- **Поддержки повторного использования**: структуры `InstructionBlock`, `LLMRequest`, `Chat`, `CompanyChat` легко масштабируются и тестируются.\n",
    "- **Расширяемости**: добавление новых типов инструкций (например, тональность, оценка качества и т.п) - это лишь вопрос дополнительных полей инструкции."
   ],
   "id": "98b7d12e32d1677"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. Начало работы\n",
    "- Из директории ```input_data```, содержащую в себе .docx файлы, необходимо извлечь данные.\n",
    "- С извлечением поможет библиотека ```python_docx```.\n",
    "- Для извлечения опишу функцию, которая вернет сырые данные (возможно пригодится в плане гибкости)."
   ],
   "id": "9ffd3f566b532893"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T13:44:29.170514Z",
     "start_time": "2025-05-07T13:44:28.074562Z"
    }
   },
   "cell_type": "code",
   "source": "%pip install python-docx",
   "id": "517a8bc17d59b5c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in c:\\users\\admin\\pycharmprojects\\pratctice\\.venv\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\admin\\pycharmprojects\\pratctice\\.venv\\lib\\site-packages (from python-docx) (5.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\admin\\pycharmprojects\\pratctice\\.venv\\lib\\site-packages (from python-docx) (4.13.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T13:44:29.198377Z",
     "start_time": "2025-05-07T13:44:29.176902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from os import listdir, path\n",
    "from docx import Document\n",
    "\n",
    "\n",
    "def load_docx_from_project(path_in_project='input_data/'):\n",
    "    documents = []\n",
    "\n",
    "    for filename in listdir(path_in_project):\n",
    "        if filename.endswith('.docx'):\n",
    "            full_path = path.join(path_in_project, filename)\n",
    "            try:\n",
    "                document = Document(full_path)\n",
    "                documents.append([filename, document])\n",
    "            except Exception as e:\n",
    "                print(f\"Не удалось загрузить {filename}: {e}\")\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "# Извлечение данных из файлов .docx\n",
    "docs = load_docx_from_project()\n"
   ],
   "id": "33fd21418cee50e",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Обработка входных данных\n",
    "- Входные данные представлены в формате .docx -> их извлечение дает свой особый формат.\n",
    "- Необходимо обработать данные путем прохода по строкам и столбцам таблицы. Также нужно очистить данные от мусора для ИИ.\n",
    "- Для обработки следует создать 3 класса: Message, Chat и Company_Chat:\n",
    "    - Message хранит в себе отправитель сообщения и его текст.\n",
    "    - Chat содержит в себе массив Messageй, имя компании, и номера обращений (возможно понадобятся в плане гибкости).\n",
    "    - CompanyChat - очищенный чат с компанией (название, переписка).\n",
    "- После обработки хорошим решением кажется создания директории очищенных .txt файлов."
   ],
   "id": "34b60c7ceb302418"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Создание классов ```Message```, ```Chat```, ```Company_Chat```.",
   "id": "5d48278cb7e465c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T13:44:29.218016Z",
     "start_time": "2025-05-07T13:44:29.213127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Message:\n",
    "    sender: str\n",
    "    text: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Chat:\n",
    "    messages: list[Message]\n",
    "    name: str = None\n",
    "    numbers: list[str] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CompanyChat:\n",
    "    company: str\n",
    "    whole_chat: str\n"
   ],
   "id": "712007a1246bb310",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Конвертация данных из .docx в Chat.",
   "id": "1c1604b84e70ba00"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T13:44:29.287032Z",
     "start_time": "2025-05-07T13:44:29.231392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def convert_from_docx(document: tuple[str, Document]):\n",
    "    name = document[0][:-5]\n",
    "\n",
    "    table = document[1].tables[0]\n",
    "    numbers: list[str] = []\n",
    "    messages: list[Message] = []\n",
    "    for row in table.rows[1:]:\n",
    "        cells = row.cells\n",
    "        message = Message(cells[1].text, cells[2].text)\n",
    "\n",
    "        numbers.append(cells[0].text)\n",
    "        messages.append(message)\n",
    "    numbers = set(numbers)\n",
    "\n",
    "    chat = Chat(\n",
    "        name=name,\n",
    "        numbers=numbers,\n",
    "        messages=messages\n",
    "    )\n",
    "    '''\n",
    "    print(f\"|{'Компания':<16}|\", chat.name,\n",
    "          f\"\\n|{'№ обращений':<16}|\", chat.numbers,\n",
    "          f\"\\n|{'Отправитель':<16}|\", chat.messages[0].sender,\n",
    "          f\"\\n|{'Текст сообщения':<16}|\\n\", chat.messages[0].text)\n",
    "    '''\n",
    "    return chat\n",
    "\n",
    "\n",
    "# Изменение типа переменных\n",
    "chats: list[Chat] = []\n",
    "for doc in docs:\n",
    "    chat = convert_from_docx(doc)\n",
    "    chats.append(chat)\n"
   ],
   "id": "e30f27a245f4eff3",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Очистка Chatов, сохранение в массив CompanyChat.",
   "id": "49c0fbb08e7d2c9f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T13:44:29.310423Z",
     "start_time": "2025-05-07T13:44:29.298022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from re import sub\n",
    "\n",
    "\n",
    "REMOVE_CHARS = {\n",
    "    0x00A0: None,  # NBSP – неразрывный пробел\n",
    "    0x00AD: None,  # SHY – мягкий перенос\n",
    "    0x200B: None,  # ZWSP – нулевой пробел\n",
    "    0xFEFF: None   # BOM – метка порядка байтов\n",
    "}\n",
    "\n",
    "\n",
    "def clear_text(\n",
    "    text: str,\n",
    "    symbols_to_remove: dict[int, None],\n",
    "    to_lower: bool = False,\n",
    "    normalize_spaces: bool = True\n",
    ") -> str:\n",
    "    text = text.translate(symbols_to_remove)\n",
    "    text = sub(r'\\n+', '\\n', text)\n",
    "    text = sub(r'[ \\t]+\\n', '\\n', text)\n",
    "\n",
    "    if normalize_spaces:\n",
    "        text = sub(r'[ \\t]{2,}', ' ', text)\n",
    "\n",
    "    text = \"\\n\".join(line.strip() for line in text.splitlines())\n",
    "\n",
    "    if to_lower:\n",
    "        text = text.lower()\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def format_chat(chat: Chat) -> CompanyChat:\n",
    "    dialog = \"\"\n",
    "\n",
    "    for message in chat.messages:\n",
    "        cleared_text = clear_text(message.text, REMOVE_CHARS)\n",
    "        dialog += (message.sender + ': ' + '\\n' +\n",
    "                   cleared_text + '\\n\\n')\n",
    "\n",
    "    name_data = CompanyChat(chat.name, dialog)\n",
    "    return name_data\n",
    "\n",
    "\n",
    "texts_to_process: list[CompanyChat] = []\n",
    "for chat in chats:\n",
    "    texts_to_process.append(format_chat(chat))\n"
   ],
   "id": "19083d80bd2f4f50",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Подготовка директории и загрузка в нее готовых данных.",
   "id": "e2d931172e80091d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T13:44:29.327017Z",
     "start_time": "2025-05-07T13:44:29.321955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from os import makedirs\n",
    "\n",
    "\n",
    "def ensure_folder_exists(folder_path: str = \"to_process\") -> None:\n",
    "    makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "\n",
    "def write_text_to_file(\n",
    "    data: CompanyChat,\n",
    "    folder_path: str = \"to_process\"\n",
    ") -> None:\n",
    "    file_path = folder_path + '/' + data.company + \".txt\"\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(data.whole_chat)\n",
    "\n",
    "\n",
    "ensure_folder_exists()\n",
    "for data in texts_to_process:\n",
    "    write_text_to_file(data)\n"
   ],
   "id": "87b8e12ee34477cf",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Основной фрагмент\n",
    "Данные очищены, отформатированы и лежат на низком старте. Для их обработки нужно формировать запросы к ИИ. Запросы будут формироваться из инструкций и переписки с ТП. Для всего озвученного целесообразно снова прибегнуть к классам:\n",
    "- ```InstructionBlock```    - составной блок инструкций для ИИ;\n",
    "- ```LLMRequest```          - запрос из инструкций и задачи;\n",
    "- ```AIModelAPI```          - API к модели ИИ;\n",
    "- ```ProblemWithSolutin```  - данные на выход (компания, краткое описание проблемы, ключевые слова, структурированное решение).\n",
    "\n",
    "Решение будет разбито на пару шагов:\n",
    "- До отправки запрос пройдет фазу сборки из инструкции и текста чата. Формирование инструкции лучше всего производить через .json файл, такой подход:\n",
    "     - Увеличивает мобильность и гибкость кода.\n",
    "     - Упрощает коррекцию инструкций.\n",
    "     - Уменьшает и упрощает код.\n",
    "\n",
    "- По API будут параллельно отправлены 3 запроса к ИИ (DeepSeek V3/ ChatGPT 4 turbo):\n",
    "    - Формирование краткого описания проблемы.\n",
    "    - Извлечение ключевых слов.\n",
    "    - Поиск и создание решения.\n",
    "\n",
    "- Переменные класса ProblemWithSolution будут сформированы из названия файла, хранящего переписку с ТП, и из ответов на запросы."
   ],
   "id": "9a635759da6fae1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Создание классов - ```InstructionBlock```, ```LLMRequest```, ```AIModelAPI```, ```ProblemWithSolutin```.",
   "id": "a0ff5498e4ca7d2a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T13:44:30.247375Z",
     "start_time": "2025-05-07T13:44:29.337912Z"
    }
   },
   "cell_type": "code",
   "source": "%pip install openai",
   "id": "de760091cc6825ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\admin\\pycharmprojects\\pratctice\\.venv\\lib\\site-packages (1.77.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\admin\\pycharmprojects\\pratctice\\.venv\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\admin\\pycharmprojects\\pratctice\\.venv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\admin\\pycharmprojects\\pratctice\\.venv\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\admin\\pycharmprojects\\pratctice\\.venv\\lib\\site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\admin\\pycharmprojects\\pratctice\\.venv\\lib\\site-packages (from openai) (2.11.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\admin\\pycharmprojects\\pratctice\\.venv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\admin\\pycharmprojects\\pratctice\\.venv\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\admin\\pycharmprojects\\pratctice\\.venv\\lib\\site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\admin\\pycharmprojects\\pratctice\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\admin\\pycharmprojects\\pratctice\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\admin\\pycharmprojects\\pratctice\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\admin\\pycharmprojects\\pratctice\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\admin\\pycharmprojects\\pratctice\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\admin\\pycharmprojects\\pratctice\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\admin\\pycharmprojects\\pratctice\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\pycharmprojects\\pratctice\\.venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T13:44:30.267114Z",
     "start_time": "2025-05-07T13:44:30.261104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from json import load\n",
    "from dataclasses import dataclass\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class InstructionBlock:\n",
    "    role: str\n",
    "    instruction: str\n",
    "    context: str\n",
    "    format: str | None\n",
    "    max_tokens: int\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LLMRequest:\n",
    "    instruction_block: InstructionBlock\n",
    "    task: str\n",
    "\n",
    "    def to_prompt(self) -> list[dict[str, str]]:\n",
    "        prompt = (\n",
    "            f\"{self.instruction_block.instruction}\\n\\n\"\n",
    "            f\"Контекст:\\n{self.instruction_block.context}\\n\\n\"\n",
    "            f\"Формат вывода:\\n{self.instruction_block.format}\\n\\n\"\n",
    "            f\"Ограничение по токенам:\\n{self.instruction_block.max_tokens}\\n\\n\"\n",
    "            f\"Входные данные:\\n{self.task}\"\n",
    "        )\n",
    "        return [\n",
    "            {\"role\": \"system\", \"content\": self.instruction_block.role},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def max_tokens(self) -> int:\n",
    "        return self.instruction_block.max_tokens\n",
    "\n",
    "\n",
    "class AIModelAPI:\n",
    "    def __init__(self, api: str, url: str, model_name: str):\n",
    "        self.api = api\n",
    "        self.url = url\n",
    "        self.model_name = model_name\n",
    "        self.client = OpenAI(api_key=self.api, base_url=self.url)\n",
    "\n",
    "    def get_response(self, request: LLMRequest,\n",
    "                     max_tokens: int = 50, temperature: float = 0.1):\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=request.to_prompt(),\n",
    "            stream=False,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ProblemWithSolution:\n",
    "        description: str\n",
    "        keywords: list[str]\n",
    "        solution: str\n",
    "        name: str = \"untitled\"\n",
    "        numbers: list[str] = None\n"
   ],
   "id": "f1bbac90040e184a",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Извлечение данных .txt, .json (переписки ТП, конфигурация инструкций). Формирование инструкций.",
   "id": "fc66b6317ad63737"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T13:44:30.289518Z",
     "start_time": "2025-05-07T13:44:30.285395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data(instruction_path: str) -> str:\n",
    "    with open(instruction_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def load_instruction_file(json_path: str):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = load(f)\n",
    "\n",
    "    abbreviations = \"\\n\".join(data[\"context\"])\n",
    "    role = data[\"role\"]\n",
    "\n",
    "    def make_instruction(key: str) -> InstructionBlock:\n",
    "        item = data[\"instructions\"][key]\n",
    "        return InstructionBlock(\n",
    "            role=role,\n",
    "            instruction=item[\"instruction\"],\n",
    "            context=abbreviations,\n",
    "            format=item[\"response_format\"],\n",
    "            max_tokens=item[\"max_tokens\"]\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"abbreviations\": abbreviations,\n",
    "        \"instruction_for_description\": make_instruction(\"description\"),\n",
    "        \"instruction_for_keywords\": make_instruction(\"keywords\"),\n",
    "        \"instruction_for_solution\": make_instruction(\"solution\"),\n",
    "    }"
   ],
   "id": "23f3b74c46df54ba",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Основная функция формирующая 3 инструктивных запроса с задачей и возвращающая переменную класса решения.",
   "id": "168da4ce46481e16"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T13:44:30.305786Z",
     "start_time": "2025-05-07T13:44:30.299931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "  \n",
    "def chat_process(\n",
    "        model: AIModelAPI,\n",
    "        path_to_instruct_json: str,\n",
    "        path_to_task_txt: str,\n",
    "        multi_thread: bool = False\n",
    ") -> ProblemWithSolution:\n",
    "\n",
    "    def _call_request(request: LLMRequest) -> str:\n",
    "        result = model.get_response(request, max_tokens=request.max_tokens)\n",
    "        return result\n",
    "\n",
    "    def _check_json_and_txt(path_json: str, path_to_txt: str,) -> None:\n",
    "        shaped_path_task = path.splitext(path.basename(path_to_txt))\n",
    "        format_of_file_task = shaped_path_task[1]\n",
    "        if format_of_file_task != \".txt\":\n",
    "            raise ValueError(f\"Ожидаемый формат файла задачи - .txt, получен - {format_of_file_task}\")\n",
    "        \n",
    "        shaped_path_instruct = path.splitext(path.basename(path_json))\n",
    "        format_of_file_instruct = shaped_path_instruct[1]\n",
    "        if format_of_file_instruct != \".json\":\n",
    "            raise ValueError(f\"Ожидаемый формат файла инструкций - .json, получен - {format_of_file_task}\")\n",
    "    \n",
    "    \n",
    "    _check_json_and_txt(path_json=path_to_instruct_json, path_to_txt=path_to_task_txt)\n",
    "    \n",
    "    name = path.splitext(path.basename(path_to_task_txt))[0]\n",
    "    \n",
    "    general_task = load_data(path_to_task_txt)\n",
    "\n",
    "    instructions = load_instruction_file(path_to_instruct_json)\n",
    "    i_description = instructions[\"instruction_for_description\"]\n",
    "    i_keywords    = instructions[\"instruction_for_keywords\"]\n",
    "    i_solution    = instructions[\"instruction_for_solution\"]\n",
    "\n",
    "    r_description = LLMRequest(i_description, task=general_task)\n",
    "    r_keywords = LLMRequest(i_keywords,       task=general_task)\n",
    "    r_solution = LLMRequest(i_solution,       task=general_task)\n",
    "\n",
    "    tasks = {\n",
    "        \"description\": r_description,\n",
    "        \"keywords\":    r_keywords,\n",
    "        \"solution\":    r_solution\n",
    "    }\n",
    "\n",
    "    results: dict[str, str] = {}\n",
    "\n",
    "    if multi_thread:\n",
    "        with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "            future_to_label = {\n",
    "                executor.submit(_call_request, request): label\n",
    "                for label, request in tasks.items()\n",
    "            }\n",
    "            for future in as_completed(future_to_label):\n",
    "                label = future_to_label[future]\n",
    "                try:\n",
    "                    results[label] = future.result(timeout=30)\n",
    "                except Exception as e:\n",
    "                    results[label] = f\"Ошибка: {e}\"\n",
    "\n",
    "    else:\n",
    "        for label, request in tasks.items():\n",
    "            try:\n",
    "                results[label] = _call_request(request)\n",
    "            except Exception as e:\n",
    "                results[label] = f\"Ошибка: {e}\"\n",
    "\n",
    "\n",
    "    for key in tasks: results.setdefault(key, \"\")\n",
    "\n",
    "    return ProblemWithSolution(\n",
    "        name=name,\n",
    "        description=results[\"description\"],\n",
    "        keywords=results[\"keywords\"].split(),\n",
    "        solution=results[\"solution\"]\n",
    "    )\n"
   ],
   "id": "9463f57f95afcdb5",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. Загрузка данных и библиотек и тестовый запуск.",
   "id": "722b7e6225a7e36"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Загрузка конфигурационных данных.",
   "id": "8df223d0a137b090"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T13:44:30.426750Z",
     "start_time": "2025-05-07T13:44:30.315982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from os import getenv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "path_to_task_txt = \"to_process/ПАКС ООО.txt\"\n",
    "path_to_instruct_json = \"prompt_data.json\"\n",
    "\n",
    "model_url = \"https://api.deepseek.com\"\n",
    "model_name = \"deepseek-chat\"\n",
    "API_AI = getenv(\"API_DS\")\n",
    "\n",
    "# model_url = \"https://api.openai.com/v1\"\n",
    "# model_name = \"gpt-4-turbo\"\n",
    "# API_AI = getenv(\"API_GPT\")\n",
    "\n",
    "model = AIModelAPI(API_AI, model_url, model_name)\n"
   ],
   "id": "42557b820912b85b",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Запуск и отладочный вывод.",
   "id": "26e3860808cdbe19"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T13:44:44.725016Z",
     "start_time": "2025-05-07T13:44:30.439778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sol = chat_process(\n",
    "    model=model,\n",
    "    path_to_instruct_json=path_to_instruct_json,\n",
    "    path_to_task_txt=path_to_task_txt,\n",
    "    multi_thread=False\n",
    ")\n",
    "\n",
    "separator = \"-\" * 50\n",
    "\n",
    "print(f\"{sol.name}              \\n{separator}\\n\"\n",
    "      f\"{sol.description}       \\n{separator}\\n\"\n",
    "      f\"{' '.join(sol.keywords)}\\n{separator}\\n\"\n",
    "      f\"{sol.solution}\"                         )"
   ],
   "id": "278468bc775fa838",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5. Описание единой функции будет создано в отдельном проекте.",
   "id": "3ae3c6801badefff"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
